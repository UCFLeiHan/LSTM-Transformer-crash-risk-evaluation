{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated methods:\n",
    "Logistic Regression (LR), Supprt Vector Machine (SVM), XGBoost, and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lei Han\\.conda\\envs\\tf\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\Lei Han\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Train on 686 samples, validate on 172 samples\n",
      "Epoch 1/20\n",
      "576/686 [========================>.....] - ETA: 0s - loss: 0.7147 - auc: 0.5313"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lei Han\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686/686 [==============================] - 2s 3ms/sample - loss: 0.7189 - auc: 0.5304 - val_loss: 0.7018 - val_auc: 0.7136\n",
      "Epoch 2/20\n",
      "686/686 [==============================] - 0s 297us/sample - loss: 0.6598 - auc: 0.6327 - val_loss: 0.6805 - val_auc: 0.7201\n",
      "Epoch 3/20\n",
      "686/686 [==============================] - 0s 316us/sample - loss: 0.6137 - auc: 0.7081 - val_loss: 0.6804 - val_auc: 0.7325\n",
      "Epoch 4/20\n",
      "686/686 [==============================] - 0s 321us/sample - loss: 0.6254 - auc: 0.6796 - val_loss: 0.6612 - val_auc: 0.7311\n",
      "Epoch 5/20\n",
      "686/686 [==============================] - 0s 319us/sample - loss: 0.5855 - auc: 0.7280 - val_loss: 0.6527 - val_auc: 0.7342\n",
      "Epoch 6/20\n",
      "686/686 [==============================] - 0s 337us/sample - loss: 0.5727 - auc: 0.7398 - val_loss: 0.6733 - val_auc: 0.7279\n",
      "Epoch 7/20\n",
      "686/686 [==============================] - 0s 344us/sample - loss: 0.5526 - auc: 0.7727 - val_loss: 0.6328 - val_auc: 0.7394\n",
      "Epoch 8/20\n",
      "686/686 [==============================] - 0s 363us/sample - loss: 0.5698 - auc: 0.7470 - val_loss: 0.6356 - val_auc: 0.7322\n",
      "Epoch 9/20\n",
      "686/686 [==============================] - 0s 340us/sample - loss: 0.5411 - auc: 0.7753 - val_loss: 0.6542 - val_auc: 0.7277\n",
      "Epoch 10/20\n",
      "686/686 [==============================] - 0s 341us/sample - loss: 0.5150 - auc: 0.8119 - val_loss: 0.6893 - val_auc: 0.7299\n",
      "Epoch 11/20\n",
      "686/686 [==============================] - 0s 335us/sample - loss: 0.5110 - auc: 0.8129 - val_loss: 0.6519 - val_auc: 0.7306\n",
      "Epoch 12/20\n",
      "686/686 [==============================] - 0s 341us/sample - loss: 0.5065 - auc: 0.8107 - val_loss: 0.6604 - val_auc: 0.7261\n",
      "Epoch 13/20\n",
      "686/686 [==============================] - 0s 339us/sample - loss: 0.5007 - auc: 0.8147 - val_loss: 0.7131 - val_auc: 0.7278\n",
      "Epoch 14/20\n",
      "686/686 [==============================] - 0s 315us/sample - loss: 0.4953 - auc: 0.8125 - val_loss: 0.6300 - val_auc: 0.7389\n",
      "Epoch 15/20\n",
      "686/686 [==============================] - 0s 317us/sample - loss: 0.5096 - auc: 0.8060 - val_loss: 0.7626 - val_auc: 0.7268\n",
      "Epoch 16/20\n",
      "686/686 [==============================] - 0s 346us/sample - loss: 0.4860 - auc: 0.8273 - val_loss: 0.7497 - val_auc: 0.7281\n",
      "Epoch 17/20\n",
      "686/686 [==============================] - 0s 335us/sample - loss: 0.4790 - auc: 0.8314 - val_loss: 0.7022 - val_auc: 0.7283\n",
      "Epoch 18/20\n",
      "686/686 [==============================] - 0s 345us/sample - loss: 0.4525 - auc: 0.8631 - val_loss: 0.6378 - val_auc: 0.7335\n",
      "Epoch 19/20\n",
      "686/686 [==============================] - 0s 343us/sample - loss: 0.4511 - auc: 0.8544 - val_loss: 0.6661 - val_auc: 0.7323\n",
      "Epoch 20/20\n",
      "686/686 [==============================] - 0s 338us/sample - loss: 0.4324 - auc: 0.8761 - val_loss: 0.6252 - val_auc: 0.7326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lei Han\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Threshold  Accuracy    Recall  Actual FAR       AUC\n",
      "0  Logistic Regression   0.571272  0.720109  0.421053    0.202055  0.642394\n",
      "1                  SVM   0.202664  0.752717  0.565789    0.198630  0.741348\n",
      "2              XGBoost   0.053165  0.747283  0.605263    0.215753  0.764014\n",
      "3                 LSTM   0.619676  0.744565  0.539474    0.202055  0.742340\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Disable TensorFlow v2 behavior\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "# Load and preprocess data\n",
    "tfrdh_train_path = '../data/Traffic_flow_and_risky_driving_behavior_train.csv'\n",
    "tfrdh_test_path = '../data/Traffic_flow_and_risky_driving_behavior_test.csv'\n",
    "tfrdh_train_data = pd.read_csv(tfrdh_train_path)\n",
    "tfrdh_test_data = pd.read_csv(tfrdh_test_path)\n",
    "\n",
    "train_X = tfrdh_train_data.drop(['Crash','event_id'], axis=1)\n",
    "train_y = tfrdh_train_data['Crash']\n",
    "test_X = tfrdh_test_data.drop(['Crash','event_id'], axis=1)\n",
    "test_y = tfrdh_test_data['Crash']\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_y), y=train_y)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Train traditional ML models\n",
    "logistic_model = LogisticRegression(class_weight=class_weights_dict)\n",
    "svm_model = SVC(class_weight=class_weights_dict, probability=True)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=class_weights[1])\n",
    "\n",
    "logistic_model.fit(train_X, train_y)\n",
    "svm_model.fit(train_X, train_y)\n",
    "xgb_model.fit(train_X, train_y)\n",
    "\n",
    "def find_threshold_for_far(y_true, y_scores, target_far):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    closest_far_index = np.argmin(np.abs(fpr - target_far))\n",
    "    return thresholds[closest_far_index]\n",
    "\n",
    "def calculate_metrics_with_threshold(model, X, y_true, target_far, is_lstm=False):\n",
    "    if is_lstm:\n",
    "        y_scores = model.predict(X).flatten()\n",
    "    else:\n",
    "        y_scores = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    threshold = find_threshold_for_far(y_true, y_scores, target_far)\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    actual_far = fp / (fp + tn)\n",
    "\n",
    "    return {\n",
    "        \"Threshold\": threshold,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Recall\": recall,\n",
    "        \"Actual FAR\": actual_far,\n",
    "        \"AUC\": auc\n",
    "    }\n",
    "\n",
    "# Evaluate traditional ML models\n",
    "metrics_logistic = calculate_metrics_with_threshold(logistic_model, test_X, test_y, 0.2)\n",
    "metrics_svm = calculate_metrics_with_threshold(svm_model, test_X, test_y, 0.2)\n",
    "metrics_xgb = calculate_metrics_with_threshold(xgb_model, test_X, test_y, 0.2)\n",
    "\n",
    "# Data processing for LSTM model\n",
    "n_hours = 5\n",
    "n_features = 18 + 24  # 18 traffic flow variables and 24 risky driving behavior variables\n",
    "n_obs = n_hours * n_features\n",
    "\n",
    "train_X = train_X.to_numpy()\n",
    "test_X = test_X.to_numpy()\n",
    "\n",
    "train_X = train_X.reshape((-1, n_hours, n_features))\n",
    "test_X = test_X.reshape((-1, n_hours, n_features))\n",
    "\n",
    "train_y = train_y.to_numpy()\n",
    "test_y = test_y.to_numpy()\n",
    "\n",
    "# Compute class weights for LSTM model\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_y), y=train_y)\n",
    "class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1 - y_true) * class_weights_dict[0] + y_true * class_weights_dict[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights * bce, axis=-1)\n",
    "\n",
    "# LSTM model\n",
    "lstm_input = Input(shape=(train_X.shape[1], train_X.shape[2]))\n",
    "lstm_out = layers.LSTM(256, return_sequences=False)(lstm_input)\n",
    "dropout = layers.Dropout(0.5)(lstm_out)\n",
    "lstm_final = layers.Dense(1, activation='sigmoid')(dropout)\n",
    "lstm_model = Model(lstm_input, lstm_final)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "lstm_model.compile(optimizer=optimizer, loss=mortality_loss, metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "lstm_model.fit(train_X, train_y, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate LSTM model with custom threshold\n",
    "metrics_lstm = calculate_metrics_with_threshold(lstm_model, test_X, test_y, 0.2, is_lstm=True)\n",
    "\n",
    "# Combine results into a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"SVM\", \"XGBoost\", \"LSTM\"],\n",
    "    \"Threshold\": [metrics_logistic[\"Threshold\"], metrics_svm[\"Threshold\"], metrics_xgb[\"Threshold\"], metrics_lstm[\"Threshold\"]],\n",
    "    \"Accuracy\": [metrics_logistic[\"Accuracy\"], metrics_svm[\"Accuracy\"],  metrics_xgb[\"Accuracy\"], metrics_lstm[\"Accuracy\"]],\n",
    "    \"Recall\": [metrics_logistic[\"Recall\"], metrics_svm[\"Recall\"],  metrics_xgb[\"Recall\"], metrics_lstm[\"Recall\"]],\n",
    "    \"Actual FAR\": [metrics_logistic[\"Actual FAR\"], metrics_svm[\"Actual FAR\"],  metrics_xgb[\"Actual FAR\"], metrics_lstm[\"Actual FAR\"]],\n",
    "    \"AUC\": [metrics_logistic[\"AUC\"], metrics_svm[\"AUC\"], metrics_xgb[\"AUC\"], metrics_lstm[\"AUC\"]]\n",
    "})\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1df98cf7124a9c068624c2f32dea9ebb1a20f188eddfe897b06a18a7ada8e82a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
